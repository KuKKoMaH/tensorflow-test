<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Title</title>
</head>
<body>
<div id="webcam-frame">
  <div id="result"></div>
  <video id="webcam" width="224px" height="224px" autoplay playsinline muted></video>
</div>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.1/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8/dist/teachablemachine-image.min.js"></script>
<script type="text/javascript">
  // More API functions here:
  // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/image

  const INPUT_NODE_NAME = 'input';
  const OUTPUT_NODE_NAME = 'MobilenetV1/Predictions/Reshape_1';


  const webcam = document.getElementById('webcam');
  const resultEl = document.getElementById('result');

  const initWebcam = () => new Promise(( resolve, reject ) => {
    const navigatorAny = navigator;
    const adjustVideoSize = ( width, height ) => {
      const aspectRatio = width / height;
      if (width >= height) {
        webcam.width = aspectRatio * webcam.height;
      } else if (width < height) {
        webcam.height = webcam.width / aspectRatio;
      }
    };
    // navigator.getUserMedia = navigator.getUserMedia ||
    //   navigatorAny.webkitGetUserMedia || navigatorAny.mozGetUserMedia ||
    //   navigatorAny.msGetUserMedia;

    if (navigator.getUserMedia) {
      navigator.getUserMedia(
        { 'video': { facingMode: 'user', width: 224, height: 224 }, 'audio': false },
        stream => {
          webcam.srcObject = stream;
          webcam.addEventListener('loadeddata', () => {
            adjustVideoSize(
              webcam.videoWidth,
              webcam.videoHeight);
            resolve();
          }, false);
        },
        reject);
    } else {
      reject('navigator.getUserMedia not found');
    }
  });

  initWebcam()
    .then(() => tf.loadGraphModel("cats_and_dogs_imagenet_js/model.json"))
    .then(model => {

      const predict = () => tf.tidy(() => {
        const webcamImage = tf.browser.fromPixels(webcam);
        const batchedImage = webcamImage.expandDims(0);
        const input = batchedImage.toFloat().div(tf.scalar(127)).sub(tf.scalar(1));

        const result = model.predict(input);
        const predictions = result.dataSync();
        console.log(predictions);
        resultEl.innerText = predictions[0] > .5 ? 'кисулькен' : 'не кисулькен';

        // requestAnimationFrame(predict);
        setTimeout(predict, 1000);
      });
      predict();
    })
    .catch(e => console.error(e));
  // let model, webcam, labelContainer, maxPredictions;
  //
  // // Load the image model and setup the webcam
  // async function init() {
  //   const modelURL = URL + "model.json";
  //   const metadataURL = URL + "metadata.json";
  //
  //   // load the model and metadata
  //   // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
  //   // or files from your local hard drive
  //   // Note: the pose library adds "tmImage" object to your window (window.tmImage)
  //   model = await tmImage.load(modelURL, metadataURL);
  //   maxPredictions = model.getTotalClasses();
  //
  //   // Convenience function to setup a webcam
  //   const flip = true; // whether to flip the webcam
  //   webcam = new tmImage.Webcam(200, 200, flip); // width, height, flip
  //   await webcam.setup(); // request access to the webcam
  //   await webcam.play();
  //   window.requestAnimationFrame(loop);
  //
  //   // append elements to the DOM
  //   document.getElementById("webcam-container").appendChild(webcam.canvas);
  //   labelContainer = document.getElementById("label-container");
  //   for (let i = 0; i < maxPredictions; i++) { // and class labels
  //     labelContainer.appendChild(document.createElement("div"));
  //   }
  // }
  //
  // async function loop() {
  //   webcam.update(); // update the webcam frame
  //   await predict();
  //   window.requestAnimationFrame(loop);
  // }
  //
  // // run the webcam image through the image model
  // async function predict() {
  //   // predict can take in an image, video or canvas html element
  //   const prediction = await model.predict(webcam.canvas);
  //   for (let i = 0; i < maxPredictions; i++) {
  //     const classPrediction =
  //             prediction[i].className + ": " + prediction[i].probability.toFixed(2);
  //     labelContainer.childNodes[i].innerHTML = classPrediction;
  //   }
  // }
</script>
</body>
</html>
